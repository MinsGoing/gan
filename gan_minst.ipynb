{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ee7196ed05de>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting F:/Python/Xiaojiang/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting F:/Python/Xiaojiang/data/train-labels-idx1-ubyte.gz\n",
      "Extracting F:/Python/Xiaojiang/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting F:/Python/Xiaojiang/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets('/data')\n",
    "#读取本地的mnist数据集\n",
    "mnist = input_data.read_data_sets(r'F:/Python/Xiaojiang/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  真实数据和噪音数据\n",
    "def get_inputs(real_size,noise_size):\n",
    "    real_img = tf.placeholder(tf.float32,[None,real_size])\n",
    "    noise_img = tf.placeholder(tf.float32,[None,noise_size])\n",
    "    return real_img,noise_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  生成器\n",
    "* noise_img:产生的噪音输入\n",
    "* n_units：隐层单元个数\n",
    "* out_dim：输出的大小（28*28*1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(noise_img,n_units,out_dim,reuse=False,alpha=0.01):\n",
    "    with tf.variable_scope(\"generator\",reuse=reuse):\n",
    "        #hidden layer\n",
    "        hidden1 = tf.layers.dense(noise_img,n_units)\n",
    "        # leaky ReLU\n",
    "        hidden1 = tf.maximum(alpha * hidden1,hidden1)\n",
    "        #dropout\n",
    "        hidden1 = tf.layers.dropout(hidden1,rate=0.2)\n",
    "        \n",
    "        # logits & outputs\n",
    "        logits = tf.layers.dense(hidden1,out_dim)\n",
    "        outputs = tf.tanh(logits)\n",
    "        \n",
    "        return logits,outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  判别器\n",
    "* img:输入\n",
    "* n_units：隐层单元个数\n",
    "* reuse：由于要使用两次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(img, n_units,reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope(\"discriminator\",reuse=reuse):\n",
    "        # hidden layer\n",
    "        hidden1 = tf.layers.dense(noise_img, n_units)\n",
    "        #leaky ReLU\n",
    "        hidden1 = tf.maximum(alpha * hidden1, hidden1)\n",
    "        \n",
    "        #logits & outputs\n",
    "        logits = tf.layers.dense(hidden1, 1)\n",
    "        outputs = tf.sigmoid(logits)\n",
    "        return logits,outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  网络参数定义\n",
    "* img_size:输入大小\n",
    "* noise_size：噪音图像大小\n",
    "* g_units：生成器隐层参数\n",
    "* d_units:判别器隐层参数\n",
    "* learning_rate:学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = mnist.train.images[0].shape[0]\n",
    "noise_size = 100\n",
    "g_units = 128\n",
    "d_units = 128\n",
    "learning_rate = 0.001\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-02243330cddf>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-02243330cddf>:8: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "real_img,noise_img = get_inputs(img_size,noise_size)\n",
    "#generator\n",
    "g_logits,g_outputs = get_generator(noise_img,g_units,img_size)\n",
    "#discriminator\n",
    "d_logits_real,d_outputs_real = get_discriminator(real_img,d_units)\n",
    "d_logits_fake,d_outputs_fake = get_discriminator(g_outputs,d_units,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator的Loss\n",
    "#识别真实图片\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                                    labels=tf.ones_like(d_logits_real)))\n",
    "#识别生成的图片\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                    labels=tf.zeros_like(d_logits_fake)))\n",
    "#总体loss\n",
    "d_loss = tf.add(d_loss_real,d_loss_fake)\n",
    "\n",
    "# generator的loss\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                               labels=tf.ones_like(d_logits_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss:\n",
      "Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "g_loss\n",
      "Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#train_vars = tf.trainable_variables()\n",
    "\n",
    "#generator\n",
    "g_vars = [var for var in tf.trainable_variables() if var.name.startswith('generator')]\n",
    "# discriminator\n",
    "d_vars = [var for var in tf.trainable_variables() if var.name.startswith('discriminator')]\n",
    "# optimizer\n",
    "print(\"d_loss:\")\n",
    "print(d_loss)\n",
    "print(\"g_loss\")\n",
    "print(g_loss)\n",
    "d_train_opt =tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt =tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=d_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/300... 判别器损失：1.4495(判别真实的：0.4795 + 判别生成的：0.9700)... 生成器损失：0.4795\n",
      "Epoch2/300... 判别器损失：1.4583(判别真实的：0.4669 + 判别生成的：0.9914)... 生成器损失：0.4669\n",
      "Epoch3/300... 判别器损失：1.4542(判别真实的：0.4700 + 判别生成的：0.9842)... 生成器损失：0.4700\n",
      "Epoch4/300... 判别器损失：1.4600(判别真实的：0.4623 + 判别生成的：0.9978)... 生成器损失：0.4623\n",
      "Epoch5/300... 判别器损失：1.4550(判别真实的：0.4697 + 判别生成的：0.9853)... 生成器损失：0.4697\n",
      "Epoch6/300... 判别器损失：1.4536(判别真实的：0.4713 + 判别生成的：0.9822)... 生成器损失：0.4713\n",
      "Epoch7/300... 判别器损失：1.4553(判别真实的：0.4676 + 判别生成的：0.9876)... 生成器损失：0.4676\n",
      "Epoch8/300... 判别器损失：1.4540(判别真实的：0.4702 + 判别生成的：0.9838)... 生成器损失：0.4702\n",
      "Epoch9/300... 判别器损失：1.4545(判别真实的：0.4705 + 判别生成的：0.9840)... 生成器损失：0.4705\n",
      "Epoch10/300... 判别器损失：1.4563(判别真实的：0.4672 + 判别生成的：0.9891)... 生成器损失：0.4672\n",
      "Epoch11/300... 判别器损失：1.4598(判别真实的：0.4626 + 判别生成的：0.9972)... 生成器损失：0.4626\n",
      "Epoch12/300... 判别器损失：1.4537(判别真实的：0.4719 + 判别生成的：0.9818)... 生成器损失：0.4719\n",
      "Epoch13/300... 判别器损失：1.4556(判别真实的：0.4663 + 判别生成的：0.9893)... 生成器损失：0.4663\n",
      "Epoch14/300... 判别器损失：1.4615(判别真实的：0.4594 + 判别生成的：1.0021)... 生成器损失：0.4594\n",
      "Epoch15/300... 判别器损失：1.4568(判别真实的：0.4680 + 判别生成的：0.9888)... 生成器损失：0.4680\n",
      "Epoch16/300... 判别器损失：1.4537(判别真实的：0.4717 + 判别生成的：0.9820)... 生成器损失：0.4717\n",
      "Epoch17/300... 判别器损失：1.4572(判别真实的：0.4663 + 判别生成的：0.9909)... 生成器损失：0.4663\n",
      "Epoch18/300... 判别器损失：1.4595(判别真实的：0.4637 + 判别生成的：0.9958)... 生成器损失：0.4637\n",
      "Epoch19/300... 判别器损失：1.4539(判别真实的：0.4700 + 判别生成的：0.9839)... 生成器损失：0.4700\n",
      "Epoch20/300... 判别器损失：1.4597(判别真实的：0.4638 + 判别生成的：0.9959)... 生成器损失：0.4638\n",
      "Epoch21/300... 判别器损失：1.4575(判别真实的：0.4673 + 判别生成的：0.9902)... 生成器损失：0.4673\n",
      "Epoch22/300... 判别器损失：1.4619(判别真实的：0.4604 + 判别生成的：1.0015)... 生成器损失：0.4604\n",
      "Epoch23/300... 判别器损失：1.4581(判别真实的：0.4640 + 判别生成的：0.9940)... 生成器损失：0.4640\n",
      "Epoch24/300... 判别器损失：1.4623(判别真实的：0.4595 + 判别生成的：1.0028)... 生成器损失：0.4595\n",
      "Epoch25/300... 判别器损失：1.4648(判别真实的：0.4553 + 判别生成的：1.0095)... 生成器损失：0.4553\n",
      "Epoch26/300... 判别器损失：1.4622(判别真实的：0.4589 + 判别生成的：1.0033)... 生成器损失：0.4589\n",
      "Epoch27/300... 判别器损失：1.4582(判别真实的：0.4654 + 判别生成的：0.9928)... 生成器损失：0.4654\n",
      "Epoch28/300... 判别器损失：1.4577(判别真实的：0.4665 + 判别生成的：0.9912)... 生成器损失：0.4665\n",
      "Epoch29/300... 判别器损失：1.4556(判别真实的：0.4689 + 判别生成的：0.9867)... 生成器损失：0.4689\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f358bf6fe67c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0msample_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         gen_samples = sess.run(get_generator(noise_img, g_units, img_size, reuse=True),\n\u001b[1;32m---> 51\u001b[1;33m                               feed_dict={noise_img:sample_noise})\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batch_size\n",
    "batch_size = 64\n",
    "epochs = 300\n",
    "# 抽取样本数\n",
    "n_sample = 25\n",
    "\n",
    "#存储测试样例\n",
    "samples = []\n",
    "#存储loss\n",
    "losses = []\n",
    "# 保存生成器变量\n",
    "saver = tf.train.Saver(var_list= g_vars)\n",
    "#开始训练\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for batch_i in range(mnist.train.num_examples//batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            batch_images = batch[0].reshape((batch_size,784))\n",
    "            # 对图像像素及逆行scale，这是因为tanh输出的结果介于(-1，1)，real和fake图片共享discriminator的参数\n",
    "            batch_images = batch_images*2 - 1\n",
    "            #genetator的输入噪声\n",
    "            batch_noise = np.random.uniform(-1, 1, size=(batch_size,noise_size))\n",
    "            #Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={real_img:batch_images, noise_img:batch_noise})\n",
    "            _ = sess.run(g_train_opt, feed_dict={noise_img: batch_noise})\n",
    "        # 每一轮结束计算loss\n",
    "        train_loss_d = sess.run(d_loss,\n",
    "                               feed_dict = {real_img:batch_images,\n",
    "                                           noise_img:batch_noise})\n",
    "        #real img loss\n",
    "        train_loss_d_real = sess.run(d_loss_real,\n",
    "                               feed_dict = {real_img:batch_images,\n",
    "                                           noise_img:batch_noise})\n",
    "        #fake img loss\n",
    "        train_loss_d_fake = sess.run(d_loss_fake,\n",
    "                               feed_dict = {real_img:batch_images,\n",
    "                                           noise_img:batch_noise})\n",
    "        #generator loss\n",
    "        train_loss_g = sess.run(g_loss,\n",
    "                               feed_dict = {real_img:batch_images,\n",
    "                                           noise_img:batch_noise})\n",
    "        print(\"Epoch{}/{}...\".format(e+1,epochs),\n",
    "             \"判别器损失：{:.4f}(判别真实的：{:.4f} + 判别生成的：{:.4f})...\".format(train_loss_d,train_loss_d_real,train_loss_d_fake),\n",
    "                \"生成器损失：{:.4f}\".format(train_loss_g))\n",
    "        losses.append((train_loss_d, train_loss_d_real, train_loss_d_fake, train_loss_g))\n",
    "\n",
    "        # 保存样本\n",
    "        sample_noise = np.random.uniform(-1, 1, size=(n_sample, noise_size))\n",
    "        gen_samples = sess.run(get_generator(noise_img, g_units, img_size, reuse=True),\n",
    "                              feed_dict={noise_img:sample_noise})\n",
    "        samples.append(gen_samples)\n",
    "\n",
    "        saver.save(sess, './checkpoint/generator.ckpt')\n",
    "#保存到本地\n",
    "with open('train_samples.pkl','wb') as f:\n",
    "    pickle.dump(samples,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
